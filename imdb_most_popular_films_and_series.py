# -*- coding: utf-8 -*-
"""Imdb Most popular Films and series

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wg2mJpAZw6cBcSMkD7kPpzovebqoI3M7

# **Imdb Most popular Films and series**

# Problem Statement
With the proliferation of streaming platforms and diverse content offerings, the demand for film and series recommendations has significantly increased. However, users often face challenges in discovering popular films and series that align with their preferences. Existing recommendation systems may not effectively utilize available data to predict user preferences or box office performance, leading to missed opportunities for content discovery and viewer engagement (Sharma et al., 2022).

# Objective

The objective of this project is to implement machine learning model (regression model and its derivative) that accurately predicts the ratings or box office performance of films and series based on various features, such as genre, release year, cast, and audience demographics. By leveraging the IMDb dataset from Kaggle, this model aims to provide insights into the factors influencing viewer preferences and help streaming platforms enhance their recommendation algorithms, ultimately improving user satisfaction and engagement with their content offerings.

# Data Collection
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

!pip install kaggle

import kagglehub

# Download latest version
path = kagglehub.dataset_download("mazenramadan/imdb-most-popular-films-and-series")

print("Path to dataset files:", path)

!kaggle datasets download -d mazenramadan/imdb-most-popular-films-and-series

import os

# List files in the dataset path
files = os.listdir(path)
print("Files in the dataset folder:", files)

# Load the CSV file
csv_file = os.path.join(path, 'imdb.csv')
df = pd.read_csv(csv_file)

# Display the first few rows
df.head()

df.info()
df.shape

"""# Data Preprocessing"""

# Check for null values in all columns
null_values_per_column = df.isnull().sum()
print("Number of missing values in each column:")
print(null_values_per_column)

df['Votes'] = df['Votes'].replace(",", "", regex=True)
df['Votes'] = pd.to_numeric(df['Votes'], errors = 'coerce')
df['Duration'] = pd.to_numeric(df['Duration'], errors = 'coerce')
df.fillna(0, inplace = True)


df['Votes'] = df['Votes'].astype('int')
df['Duration'] = df['Duration'].astype('int')

df.info()

"""##Handle missing values"""

df['Episodes'] = pd.to_numeric(df['Episodes'], errors = 'coerce')
df.fillna(0, inplace = True)
df['Episodes'] = df['Episodes'].astype('int')

# Fill missing values in numerical columns with the median
numerical_cols = ['Duration']
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())

# Fill missing values in categorical columns with the mode
categorical_cols = ['Certificate', 'Nudity', 'Violence', 'Profanity', 'Alcohol', 'Frightening']
for col in categorical_cols:
# Fill with mode and assign it back to the column
    df[col] = df[col].fillna(df[col].mode()[0])

# Check for null values in all columns after handling missing values
null_values_per_column_after = df.isnull().sum()

# Print the result
print("Number of missing values in each column after handling:")
print(null_values_per_column_after)

df.head(10)

df['Frightening'].unique()

def replace_value(df):
    df = df.replace({'None':0,'Mild':1,'Moderate':2,'Severe':3,'No Rate':0,'-':0,'No Votes':0})
    return df

df = df.apply(replace_value)

"""## Drop Duplicates Data"""

print('Total duplicates data {}'.format(df.duplicated(keep='first').sum()))
df = df.drop_duplicates(keep = 'first')

df.info()

"""# EDA

## Outliers Detection
"""

# Import necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Select all numerical columns from the dataframe
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Set up the figure size
plt.figure(figsize=(15, 8))

# Loop through each numerical column and create a box plot
for i, col in enumerate(numerical_cols):
    plt.subplot(2, len(numerical_cols)//2 + 1, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()

plt.show()

"""## Handle Outliers"""

# Calculate the IQR (Interquartile Range) for 'Votes', 'Duration'
Q1_votes = df['Votes'].quantile(0.25)
Q3_votes = df['Votes'].quantile(0.75)
IQR_votes = Q3_votes - Q1_votes

Q1_duration = df['Duration'].quantile(0.25)
Q3_duration = df['Duration'].quantile(0.75)
IQR_duration = Q3_duration - Q1_duration


# Define the upper and lower bounds for outliers in 'Votes', 'Duration'
lower_bound_votes = Q1_votes - 1.5 * IQR_votes
upper_bound_votes = Q3_votes + 1.5 * IQR_votes

lower_bound_duration = Q1_duration - 1.5 * IQR_duration
upper_bound_duration = Q3_duration + 1.5 * IQR_duration


# Filter rows where 'Votes', 'Duration' are considered outliers
votes_outliers = df[(df['Votes'] < lower_bound_votes) | (df['Votes'] > upper_bound_votes)]
duration_outliers = df[(df['Duration'] < lower_bound_duration) | (df['Duration'] > upper_bound_duration)]

# Print the number of outliers
print("Number of Votes Outliers:", len(votes_outliers))
print("Number of Duration Outliers:", len(duration_outliers))

# Remove outliers from 'Votes', 'Duration' by filtering the dataframe
df_cleaned = df[
    (df['Votes'] >= lower_bound_votes) & (df['Votes'] <= upper_bound_votes) &
    (df['Duration'] >= lower_bound_duration) & (df['Duration'] <= upper_bound_duration) ]

# Print the shape of the cleaned dataframe to see how many rows remain
print("Shape of the dataframe after cleaning:", df_cleaned.shape)

df_cleaned.info()

"""## Visualization Distribtion"""

import seaborn as sns
import matplotlib.pyplot as plt

# Define the columns to plot the distribution
columns_to_plot = ['Votes', 'Duration', 'Nudity', 'Violence', 'Profanity', 'Alcohol', 'Frightening']

# Set up the plotting area with appropriate size
plt.figure(figsize=(16, 12))

# Loop through the columns and plot the distribution
for i, col in enumerate(columns_to_plot, 1):
    plt.subplot(3, 3, i)  # Create a subplot (3x3 grid for 9 plots)

    # Plot distribution with histogram and KDE
    sns.histplot(df_cleaned[col], kde=True, bins=20)

    # Set the title for each plot
    plt.title(f'Distribution of {col}')

# Adjust layout to prevent overlapping plots
plt.tight_layout()

# Show the plots
plt.show()

print(df_cleaned.info())

"""# Correlation Map"""

df2 = df_cleaned.copy()

# Dropping unnecessary columns
df2.drop(['Name', 'Certificate', 'Type', 'Genre'], axis=1, inplace=True, errors='ignore')  # Use errors='ignore' to avoid KeyError

# Select only numerical columns
numerical_df = df2.select_dtypes(include=[np.number])

# Plotting Correlation Matrix
plt.figure(figsize=(14, 10))
correlation_matrix = df2.corr()  # Calculate the correlation matrix
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='viridis', square=True, cbar=True)
plt.title('Correlation Matrix Before One-Hot Encoding')
plt.show()

df2.head()

"""# One hot Encoding"""

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(drop='first', sparse_output=False)

df3 = df_cleaned.copy()
df3['Genre'] = df3['Genre'].str.replace(' ', '', regex=False)  # Use regex=False for safety
genre_cols = df3['Genre'].str.get_dummies(sep=',')

genre_cols

# Combine genre columns with the original dataframe
df3 = pd.concat([df3, genre_cols], axis=1, join='inner')
df3 = df3.drop(columns='Genre', errors='ignore')
df3.head()

"""# Label Encoding"""

df3['Certificate'] = df3['Certificate'].astype('string')

df3['Certificate'].unique()

from sklearn.preprocessing import LabelEncoder
# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to the 'Certificate' column
df3['Certificate_encoded'] = label_encoder.fit_transform(df3['Certificate'])

# Check the unique values and corresponding labels
certificate_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Label Encoding Mapping for 'Certificate':", certificate_mapping)

# Drop the original 'Certificate' column
df3 = df3.drop(columns='Certificate', errors='ignore')

# Apply label encoding to the 'Type' column
df3['Type_encoded'] = label_encoder.fit_transform(df3['Type'])

# Check the unique values and corresponding labels for 'Type'
type_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Label Encoding Mapping for 'Type':", type_mapping)

# Drop the original 'Type' column
df3 = df3.drop(columns='Type', errors='ignore')

df3.head()

#Drop unnecessary column
column_to_drop = ['Name']

df3 = df3.drop(columns=column_to_drop, errors='ignore')

# Display the updated DataFrame to verify the column is removed
df3.head()

check_missing = df3.isnull().sum()
print(check_missing)

"""# Model Implemmentation

# Linear Regression
"""

df_reg = df3.copy()

print(df_reg.info())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import mean_squared_error, r2_score

# Predict 'Rates' as target column
x = df_reg.drop('Rate', axis=1)  # Features
y = df_reg['Rate']  # Target variable

"""## Split train and test data"""

#Split train and test
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.33, random_state = 0)

# Check the number of rows in the train and test sets
print(f"Training set size (x_train): {x_train.shape[0]} rows")
print(f"Test set size (x_test): {x_test.shape[0]} rows")
print(f"Training set size (y_train): {y_train.shape[0]} rows")
print(f"Test set size (y_test): {y_test.shape[0]} rows")

"""## Feature Scaling"""

# Scaling Standard dization mean = 0 and std = 1.
from sklearn.preprocessing import StandardScaler
std_x = StandardScaler()
x_train = std_x.fit_transform(x_train)
x_test = std_x.transform(x_test)

from sklearn.linear_model import LinearRegression

# Initialize and fit the model
regmodel = LinearRegression()
regmodel.fit(x_train, y_train)

# Make predictions
y_pred_linear = regmodel.predict(x_test)

import pandas as pd

# Ensure y_test is numeric
y_test = pd.to_numeric(y_test, errors='coerce')

# Ensure y_pred is also numeric
y_pred_linear = np.array(y_pred_linear)

import matplotlib.pyplot as plt
import numpy as np

# Plotting Actual vs Predicted values
plt.figure(figsize=(10, 6))

# Scatter plot for actual vs predicted values
plt.scatter(y_test, y_pred_linear, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)

# Adding labels and title
plt.xlabel('Actual Rates')
plt.ylabel('Predicted Rates')
plt.title('Actual vs Predicted Rates')

# Display the plot
plt.show()

# Calculating and printing Linear Mean Squared Error
linear_mse = metrics.mean_squared_error(y_test, y_pred_linear)
print(f'Linear Mean Squared Error: {linear_mse}')

# Calculating and printing Linear R² Score
linear_r2 = metrics.r2_score(y_test, y_pred_linear)
print(f'Linear R² Score: {linear_r2}')

"""# Lasso Regression"""

df5 = df3.copy()

# Predict 'Rates' as target column
x = df5.drop('Rate', axis=1)  # Features
y = df5['Rate']  # Target variable

from sklearn.linear_model import LinearRegression, Lasso
# Create a Lasso regression model
lasso_model = Lasso(alpha=0.1)

# Fit the model
lasso_model.fit(x_train, y_train)

# Make predictions
y_pred_lasso = lasso_model.predict(x_test)

# Calculate Mean Squared Error and R² score
mse = mean_squared_error(y_test, y_pred_lasso)
r2 = r2_score(y_test, y_pred_lasso)

print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

# Plotting Actual vs Predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_lasso, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')  # Diagonal line
plt.title('Actual vs Predicted Votes (Lasso Regression)')
plt.xlabel('Actual Votes')
plt.ylabel('Predicted Votes')
plt.grid()
plt.show()

"""## Hyperparameter Fine Tunning"""

# Model Implementation with Hyperparameter Tuning

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Lasso
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# Make a copy of the original dataframe to avoid modifying it
df_par = df3.copy()

# Predict 'Rates' as target column
x = df_par.drop('Rate', axis=1)  # Features
y = df_par['Rate']  # Target variable

# Split train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)

# Scaling: Standardization (mean = 0 and std = 1)
std_x = StandardScaler()
x_train = std_x.fit_transform(x_train)
x_test = std_x.transform(x_test)

# Function to evaluate models
def evaluate_model(model, x_train, y_train, x_test, y_test):
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    mse = metrics.mean_squared_error(y_test, y_pred)
    r2 = metrics.r2_score(y_test, y_pred)
    return mse, r2

# Define hyperparameters for Lasso, Ridge, and Elastic Net
lasso_params = {'alpha': np.logspace(-4, 1, 10)}  # Alpha values for Lasso

# Lasso Regression with Grid Search
lasso_grid = GridSearchCV(Lasso(), lasso_params, scoring='neg_mean_squared_error', cv=5)
lasso_grid.fit(x_train, y_train)
best_lasso = lasso_grid.best_estimator_
lasso_mse, lasso_r2 = evaluate_model(best_lasso, x_train, y_train, x_test, y_test)

print('Best Lasso Parameters:', lasso_grid.best_params_)
print('Lasso Regression MSE:', lasso_mse)
print('Lasso Regression R²:', lasso_r2)

# Visualizing the results
models = ['Linear', 'Lasso']
mses = [linear_mse, lasso_mse]
r2_scores = [linear_r2, lasso_r2]

# Plot MSE
plt.figure(figsize=(10, 5))
plt.bar(models, mses, color='lightblue')
plt.title('Mean Squared Error Comparison')
plt.xlabel('Models')
plt.ylabel('Mean Squared Error')
plt.show()

# Plot R² Scores
plt.figure(figsize=(10, 5))
plt.bar(models, r2_scores, color='lightgreen')
plt.title('R² Score Comparison')
plt.xlabel('Models')
plt.ylabel('R² Score')
plt.show()

"""## Learning Curve"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Lasso
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# Function to calculate and plot learning curves
def plot_learning_curve(model, x_train, y_train, x_test, y_test, title):
    train_errors = []
    val_errors = []
    train_sizes = np.linspace(0.1, 1.0, 10)

    for train_size in train_sizes:
        # Use a subset of the training data
        subset_size = int(len(x_train) * train_size)
        x_train_subset = x_train[:subset_size]
        y_train_subset = y_train[:subset_size]

        # Train the model on the subset
        model.fit(x_train_subset, y_train_subset)

        # Predict on the training subset and the test set
        y_train_predict = model.predict(x_train_subset)
        y_test_predict = model.predict(x_test)

        # Calculate training and validation errors (MSE)
        train_errors.append(metrics.mean_squared_error(y_train_subset, y_train_predict))
        val_errors.append(metrics.mean_squared_error(y_test, y_test_predict))

    # Plotting the learning curve
    plt.plot(train_sizes * len(x_train), train_errors, 'b-', label="Training Error")
    plt.plot(train_sizes * len(x_train), val_errors, 'r-', label="Validation Error")
    plt.title(f'Learning Curve: {title}')
    plt.xlabel('Training Set Size')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.show()

# Make a copy of the original dataframe to avoid modifying it
df_par1 = df3.copy()

# Predict 'Rates' as target column
x = df_par1.drop('Rate', axis=1)  # Features
y = df_par1['Rate']  # Target variable

# Split train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)

# Scaling: Standardization (mean = 0 and std = 1)
std_x = StandardScaler()
x_train = std_x.fit_transform(x_train)
x_test = std_x.transform(x_test)

# Linear Regression Model
linear_model = LinearRegression()

# Lasso Regression Model with Hyperparameter Tuning
lasso_params = {'alpha': np.logspace(-4, 1, 10)}  # Alpha values for Lasso
lasso_grid = GridSearchCV(Lasso(), lasso_params, scoring='neg_mean_squared_error', cv=5)
lasso_grid.fit(x_train, y_train)
best_lasso = lasso_grid.best_estimator_

# Plot learning curve for Linear Regression
plot_learning_curve(linear_model, x_train, y_train, x_test, y_test, title="Linear Regression")

# Plot learning curve for Lasso Regression
plot_learning_curve(best_lasso, x_train, y_train, x_test, y_test, title="Lasso Regression")

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
# Make a copy of the original dataframe
df_rf = df3.copy()

# Predict 'Rates' as target column
x = df_rf.drop('Rate', axis=1)  # Features
y = df_rf['Rate']  # Target variable

# Split train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)

# Scaling: Standardization
std_x = StandardScaler()
x_train = std_x.fit_transform(x_train)
x_test = std_x.transform(x_test)

"""## Best Parameter"""

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],  # More trees
    'max_features': ['log2', 'sqrt'],  # Experiment with different features
    'max_depth': [None, 20, 30, 40],  # Allow for deeper trees
    'min_samples_split': [2, 5, 10],  # Try increasing the split criteria
    'min_samples_leaf': [1, 2, 4]  # Explore larger leaf sizes
}

# Perform Grid Search with cross-validation
rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
rf_grid.fit(x_train, y_train)

# Best estimator
best_rf_model = rf_grid.best_estimator_
print("Best Parameters:", rf_grid.best_params_)

# Predict using the best Random Forest model
y_rf_pred = best_rf_model.predict(x_test)

# Evaluate Random Forest
rf_mse = mean_squared_error(y_test, y_rf_pred)
rf_r2 = r2_score(y_test, y_rf_pred)

print(f'Random Forest Regression MSE: {rf_mse}')
print(f'Random Forest Regression R²: {rf_r2}')

# Convert y_test to float64 explicitly
y_test = y_test.astype(float)
y_rf_pred = np.array(y_rf_pred)

# Convert y_test and y_rf_pred to numpy arrays and ensure they are floats y_test = np.array(y_test)
y_rf_pred = np.array(y_rf_pred)

# Plot Actual vs Predicted
plt.figure(figsize=(10, 6))

# Scatter plot for Actual vs Predicted
plt.scatter(y_test, y_rf_pred, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # Plot a diagonal line for reference

# Adding labels and title
plt.xlabel('Actual Rates')
plt.ylabel('Predicted Rates')
plt.title('Actual vs Predicted Rates (Random Forest)')
plt.grid(True)

# Show the plot
plt.show()